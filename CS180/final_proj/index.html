<!DOCTYPE HTML>
<!--
	Future Imperfect by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Neural Radiance Field</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="./assets/css/main.css" />
		<script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
		<!-- Lightweight client-side loader that feature-detects and load polyfills only when necessary -->
		<script src="https://cdn.jsdelivr.net/npm/@webcomponents/webcomponentsjs@2/webcomponents-loader.min.js"></script>
		<script type="module" src="https://cdn.jsdelivr.net/npm/zero-md@3?register"></script>
		<script type="text/javascript" async
			src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js">
		</script>
	</head>
	<body class="single is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
				<header id="header">
					<h1><a href="https://haoyuexiao.github.io/">Haoyue Xiao</a></h1>
					<nav class="links">
						<ul>
							<li><a href="https://haoyuexiao.github.io/education/education.html">Education</a></li>
							<li><a href="https://haoyuexiao.github.io/projects/projects.html">Projects</a></li>
							<li><a href="https://haoyuexiao.github.io/cv/CV.htmll">CV</a></li>
							<li><a href="https://haoyuexiao.github.io/personal/personal.html">Personal</a></li>
						</ul>
					</nav>
					<nav class="main">
						<ul>
			
							<li class="menu">
								<a class="fa-bars" href="#menu">Menu</a>
							</li>
						</ul>
					</nav>
				</header>

				<!-- Menu -->
				<section id="menu">

					<!-- Links -->
					<section>
						<ul class="links">
							<li>
								<a href="https://haoyuexiao.github.io/education/education.html">
									<h3>Education</h3>
								</a>
							</li>
							<li>
								<a href="https://haoyuexiao.github.io/projects/projects.html">
									<h3>Projects</h3>
								</a>
							</li>
							<li>
								<a href="https://haoyuexiao.github.io/cv/CV.html">
									<h3>CV</h3>
								</a>
							</li>
							<li>
								<a href="https://haoyuexiao.github.io/personal/personal.html">
									<h3>Personal</h3>
								</a>
							</li>
						</ul>
					</section>
				
					<!-- Contents -->
					<!-- Contents -->
					<section>
						<h3>Contents</h3>
						<ul class="toc">
							<li>
								<a href="#introduction">Introduction</a>
							</li>
							<li>
								<a href="#part-1-fit-a-neural-field-to-a-2d-image">Part 1: Fit a Neural Field to a 2D Image</a>
								<ul>
									<li><a href="#implementation-details">Implementation Details</a></li>
									<li>
										<a href="#summary-of-outcomes">Summary of Outcomes</a>
										<ul>
											<li><a href="#the-fox">The fox</a></li>
											<li><a href="#the-cherry-blossom">The Cherry Blossom</a></li>
										</ul>
									</li>
									<li><a href="#hyper-parameter-tuning">Hyper Parameter Tuning</a></li>
								</ul>
							</li>
							<li>
								<a href="#part-2-fit-a-neural-radiance-field-from-multi-view-images">Part 2: Fit a Neural Radiance Field from Multi-view Images</a>
								<ul>
									<li><a href="#process-of-sampling-rays">Process of Sampling Rays</a></li>
									<li><a href="#sampling-points-along-the-rays">Sampling Points Along the Rays</a></li>
									<li><a href="#model-architecture">Model Architecture</a></li>
									<li><a href="#volume-rendering">Volume Rendering</a></li>
									<li><a href="#summary-of-outcomes">Summary of Outcomes</a></li>
								</ul>
							</li>
							<li>
								<a href="#bells-whistles">Bells &amp; Whistles</a>
								<ul>
									<li><a href="#change-background-color">Change Background Color</a></li>
									<li><a href="#rendering-the-depth-map">Rendering the Depth Map</a></li>
								</ul>
							</li>
						</ul>
					</section>

					<!-- Actions -->
					<section>
						<ul class="actions stacked">
							<li><a href="https://haoyuexiao.github.io/email_contact/email.html" class="button large fit">Contact</a></li>
						</ul>
					</section>
				
				</section>
				

				</section>

				<!-- Main -->
				<div id="main">
					<!-- Post -->
					<article class="post">
						<header>
							<div class="title">
								<h2><a href="#">Neural Radiance Field</a></h2>
							</div>
							<div class="meta">
								<time class="published" datetime="2024-12-10">December 10, 2024</time>
								<a href="#" class="author"><span class="name">Haoyue Xiao</span><img src="./images/avatar.png" alt="avatar" /></a>
							</div>
						</header>
						<div style="display: flex; align-items: center; justify-content: center;">
							<span class="image featured"><img src="./images/lego_reconstruction.gif" alt="" /></span>
						</div>
						<div style="padding-left:10px; padding-right:10px">
							<h2 id="introduction">Introduction</h2>
							<p>
								Ever wondered how we can turn regular photos into stunning 3D models? Enter NeRF - Neural Radiance Fields - a breakthrough technology that's transforming 3D content creation. At its heart, NeRF is an elegant fusion of deep learning and ray optics, representing 3D scenes not through traditional geometry, but through a neural network that maps 3D points and viewing angles to colors and densities.
							</p>
							<hr>
							<h2 id="part-1-fit-a-neural-field-to-a-2d-image">Part 1: Fit a Neural Field to a 2D Image</h2>
							<p>
								Before entering the fascinating world of Nerf, let's work on its simplified version of reconstructing a fixed 2D image. In Nerf, we will use a neural network to fit a radiance field and render images from different camera centers and viewing angles, while in the 2D case, there's no radiance or different viewing origins, so we are just predicting the RGB value of the training image given the position of the pixel, namely
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								$$
								F: [0, W]\times [0, H] \ni[u,v]^T \to \mathrm{rgb}_{u,v} \in [0,1]^3
								$$
							</div>
							<hr>
							<h3 id="implementation-details">Implementation Details</h3>
							<p>
								The model architecture is implemented following
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/p1_model.png" alt="Model Architecture" /></span>
							</div>
							<p>
								Before feeding the 2-d positional vector into the network, it must go through a positional encoding. Unlike the positional encoding used in the transformer architecture intended to give an order of tokens, here, the PE is used to expand the input to higher frequencies so the details can be well distinguished. The formula for PE is
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								$$
								\begin{equation*}
								\begin{aligned}
								\mathrm{PE}(x) &= \\
								
								&\left[x, \sin(2^0\pi x), \cos(2^0\pi x), \cdots, \sin(2^{L-1}\pi x), \cos(2^{L-1}\pi x)\right]
								\end{aligned}
								\end{equation*}
								$$
							</div>
							<p>
								I also implemented a data loader to speed up the training process, where I randomly pick \(N\) pixels from the training image in every epoch, get the pixel positions of these pixels which are normalized into \([0,1]\) scale.
							</p>
							<p>
								The model is trained with MSE loss between the predicted RGB value and the ground truth values. The final quality of the reconstructed image is measured by the <strong>Peak signal-to-noise ratio (PSNR)</strong>, defined as
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								$$
								PSNR = 10 \cdot \log_{10}\left(\frac{1}{MSE}\right)
								$$
							</div>
							<hr>
							<h3 id="summary-of-outcomes">Summary of Outcomes</h3>
							<p>
								I trained the model on two testing examples.
							</p>
							
							<h4 id = "the-fox">The fox</h4>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/fox.jpg" alt="The Fox" /></span>
							</div>
							<p>
								I used the following hyper-parameters when training:
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								$$
								\mathrm{Adam \text{ }lr} = 10^{-2} \quad L = 10 \quad \mathrm{Batch}=10000
								$$
							</div>
							<p>
								The training process versus PSNR is
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/p1_fox_psnr.png" alt="PSNR Training Curve for Fox" /></span>
							</div>
							<p>
								and the training loss against epoch
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/p1_fox_loss.png" alt="Training Loss for Fox" /></span>
							</div>
							<p>
								Across the iterations, the reconstructed images evolve like
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/fox_evolve.png" alt="Fox Evolution" /></span>
							</div>
				
							<h4 id="the-cherry-blossom">The Cherry Blossom</h4>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/cherry.JPG" alt="The Cherry Blossom" /></span>
							</div>
							<p>
								I used the following hyper-parameters when training:
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								$$
								\begin{equation*}
								\begin{aligned}
								&\mathrm{Adam \text{ }lr} = 10^{-3} \quad L = 10\\
								&\mathrm{Batch}=10000 \quad \text{epoch} = 5000
								\end{aligned}
								\end{equation*}
								$$
							</div>
							<p>
								and the training process versus PSNR is
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/p1_cherry_pnsr.png" alt="PSNR Training Curve for Cherry" /></span>
							</div>
							<p>
								and the training loss against epoch
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/p1_cherry_loss.png" alt="Training Loss for Cherry" /></span>
							</div>
							<p>
								Across the iterations, the reconstructed images evolve like
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/cherry_evolve.png" alt="Cherry Blossom Evolution" /></span>
							</div>
							<hr>
							<h3 id="hyper-parameter-tuning">Hyper Parameter Tuning</h3>
							<p>
								To explore the performance of the model under different values of embedding length \(L\) and learning rates, I compare the results of different hyperparameters and summarize them below
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/different_L_pnsr.png" alt="Different L PSNR" /></span>
							</div>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/different_L_img.png" alt="Different L Images" /></span>
							</div>
							<p>
								From the visualizations above, we see that \(L=10\) achieves the best performance. Lower \(L\) values fail to capture the high frequencies, while higher \(L\) values are difficult to converge to a good result within 3000 epochs.
							</p>
							<p>
								Fixing \(L=10\), now we compare different learning rates:
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/different_lr_pnsr.png" alt="Different Learning Rates PSNR" /></span>
							</div>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/different_lr_img.png" alt="Different Learning Rates Images" /></span>
							</div>
							<p>
								From the results above, we see that \(lr=0.01\) and \(lr=0.001\) have the best performance.
							</p>
							<hr>
							<h2 id="part-2-fit-a-neural-radiance-field-from-multi-view-images">Part 2: Fit a Neural Radiance Field from Multi-view Images</h2>
							<p>
								Now, we are ready to train the 3D case of Neural Radiance Field. Generally, given random images, we need to run structure from motion (SfM) to recover the camera positions (camera-to-world matrices).
							</p>
							<p>
								For simplicity, we will use the lego dataset of the original paper, which already has all the camera positions known. Some example images are
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/sample_lego.png" alt="Sample Lego" /></span>
							</div>
							<hr>
							<h3 id="process-of-sampling-rays">Process of Sampling Rays</h3>
							<p>
								To train the model, we need to sample rays from the images. By rays we mean a directed 3D line emitted from the camera center and passing through a pixel of the camera plane.
							</p>
							<p>
								Fix a camera, we have known intrinsic \(K\), then for a batched pixel coordinates, \(\{(u^{(p)}_i, v^{(p)}_i)\}_{i=1}^n\), the following equation is satisfied:
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								$$
								s_i\begin{bmatrix}u^{(p)}_i\\ v^{(p)}_i\\ 1\end{bmatrix} = K\begin{bmatrix}x^{(c)}_i\\ y^{(c)}_i\\ z^{(c)}_i\end{bmatrix}
								$$
							</div>
							<p>
								that is, given depth \(s\), we can obtain the corresponding point in camera coordinates using
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								$$
								\begin{bmatrix}x^{(c)}_i\\ y^{(c)}_i\\ z^{(c)}_i\end{bmatrix} = sK^{-1}\begin{bmatrix}u^{(p)}_i\\ v^{(p)}_i\\ 1\end{bmatrix}
								$$
							</div>
							<p>
								(Considering the ray emitting from the camera center and passing through the pixel at the image plane, then specifying the depth \(s\) can be seen as choosing the point along this ray with the corresponding depth in the camera coordinate system)
							</p>
							<p>
								Then, given the camera-to-world matrix
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								$$
								\mathcal{M}_{c2w} = \begin{bmatrix}
								R_{3\times 3}&t_{3\times 1}\\0_{1\times 3}&1
								\end{bmatrix}^{-1} = \begin{bmatrix}
								R^{-1}&-R^{-1}t\\0&1
								\end{bmatrix}
								$$
							</div>
							<p>
								we can convert the 3-d points in camera coordinates \(\{(x^{(c)}_i, y^{(c)}_i, z^{(c)}_i)\}_{i=1}^n\) back to the points in real-world coordinates \(\{(x^{(w)}_i, y^{(w)}_i, z^{(w)}_i)\}_{i=1}^n\) using the formula
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								$$
								\begin{bmatrix}x^{(w)}_i\\ y^{(w)}_i\\ z^{(w)}_i\end{bmatrix} = \mathcal{M}_{c2w}\begin{bmatrix}x^{(c)}_i\\ y^{(c)}_i\\ z^{(c)}_i\end{bmatrix}
								$$
							</div>
							<p>
								the camera center is calculated from \(\mathcal{M}_{c2w}\) by
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								$$
								r_o = -R^{-1}t
								$$
							</div>
							<p>
								and we can get the direction \(r_d\) by
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								$$
								r_d = \frac{x_w - r_o}{||x_w - r_o||_2}
								$$
							</div>
							<p>
								Then, the pair \((r_o, r_d)\) represents the ray
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								$$
								r = r_o + tr_d
								$$
							</div>
							<p>
								With these relations, I implemented the dataset which randomly samples <code>n_sample</code> rays at each iteration.
							</p>
							<p>
								In detail, I merged all training images to have <code>H*W*n_training</code> elements and each element representing a ray emitted from some training camera position. We can extract the correct ray by first finding the correct image using <code>idx // (H*W)</code>, then find the ray's id within that image using <code>idx % (H*W)</code>, finally, we find the correct pixel <code>(id_in_img % W, id_in_img // W)</code>. Then, we are allowed to sample <code>n_sample</code> rays at each iteration.
							</p>
							<hr>
							<h3 id="sampling-points-along-the-rays">Sampling Points Along the Rays</h3>
							<p>
								After getting the rays, we need to sample \(N_c\) points along the ray to get the RGB values at these points. For simplicity, we sample these points (\(t\) values) by uniformly choosing points from \(t \in [near, far]\) and add a small noise to it. In this project, I used \(N_c = 64\), and here are some results of sampling rays and points:
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/sampled_rays.png" alt="Sampled Rays" /></span>
							</div>
							<p>
								and the result of sampling all on one camera:
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/sampled_rays_one_cam.png" alt="Sampled Rays One Camera" /></span>
							</div>
							<p>
								According to the original paper, we can use a more clever way of sampling by sampling from the CDF of uniform sampling to sample more points with higher density.
							</p>
							<hr>
							<h3 id="model-architecture">Model Architecture</h3>
							<p>
								Similar to Part 1, we use a MLP to fit the function
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								$$
								F: \{(x, y, z), r_d\} \to \{(r,g,b), \sigma\}
								$$
							</div>
							<p>
								That is, given the position of a particle and its viewing angle, the model predicts the RGB value and density (visibility) of that point from the specified angle (when the angle changes, the point also looks different).
							</p>
							<p>
								The model is implemented according to
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/p2_model.png" alt="Model Architecture Part 2" /></span>
							</div>
							<hr> 
							<h3 id="volume-rendering">Volume Rendering</h3>
							<p>
								When doing reconstruction, given the position of a camera (<code>c2w</code> matrix) and its resolution, we use the dataloader to create query points along each ray in the image, and then use the model to predict the RGB values of points along each ray. Then, we need to integrate the color values with the density to get the RGB value from the desired viewpoint. The integration is given by
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								$$
								\begin{equation*}
								\begin{aligned}
								\hat{C}(r) = \sum_{i=1}^{N_c} T_i \left(1 - \exp\left(-\sigma_i\delta_i\right)\right)c_i
								\end{aligned}
								\end{equation*}
								$$
							</div>
							<p>where \(T_i = \exp\left(-\sum_{j=1}^{i-1}\sigma_j\delta_j\right)\)</p>
							<p>
								In our naive implementation, \(\delta\) is always a fixed value, representing discrete \(\Delta t\). Using this method, we can construct the image from any given camera position.
							</p>
							<hr> 
							<h3 id="summary-of-outcomes">Summary of Outcomes</h3>
							<p>
								I used the following settings during training:
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								$$
								\begin{equation*}
								\begin{aligned}
								&\mathrm{batch\text{ }size}: 3000 \quad \text{lr}: 5\cdot 10^{-5}\\
								&N_c: 64 \quad \text{n\(\_\)iteration}: 9000
								\end{aligned}
								\end{equation*}
								
								$$
							</div>
							<p>
								Optimizer settings:
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								$$
								\begin{equation*}
								\begin{aligned}
								\text{AdamW with } \gamma = 0.95
								\end{aligned}
								\end{equation*}
								$$
							</div>
							<p>with the CosineAnnealingLR Scheduler</p>
							<p>
								The training loss against the training process:
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/training_curve.png" alt="Training Curve" /></span>
							</div>
							<p>
								(Please ignore the "every 100 epochs" title, that's a plot mistake)
							</p>
							<p>
								The PSNR improvement against the training process:
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/PNSR.png" alt="PSNR Improvement" /></span>
							</div>
							<p>
								From the plot, we see that the final average PSNR across the 10 validation images is 25.79.
							</p>
							<p>
								Some images showing the evolution of reconstruction:
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/psnr_evolution_p2.png" alt="PSNR Evolution" /></span>
							</div>
							<p>
								The spherical rendering of lego video:
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/lego_reconstruction.gif" alt="Lego Reconstruction" /></span>
							</div>
							<hr>
							<h2 id="bells-whistles">Bells &amp; Whistles</h2>
				
							<h3 id="change-background-color">Change Background Color</h3>
							<p>
								Currently, the background of all viewing angles is rendered as black. Let's recall the volume rendering formula
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								$$
								\hat{C}(r) = \sum_{i=1}^{N_c} T_i \left(1 - \exp\left(-\sigma_i\delta_i\right)\right)c_i
								$$
							</div>
							<p>
								In this expression, \(T_i = \exp\left(-\sum_{j=1}^{i-1}\sigma_j\delta_j\right)\) represents the probability that the light didn't terminate at the previous \(i-1\) particles, while \(1 - \exp\left(-\sigma_i\delta_i\right)\) is the probability that the light terminates at the current particle. Therefore, to use a different background color other than black, consider the case that the light passes through all sampled points and goes to infinity (the background). The probability that this happens is
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								$$
								T_{N_c+1} = \exp\left(-\sum_{j=1}^{N_c}\sigma_j\delta_j\right)
								$$
							</div>
							<p>
								and if this happens, we will see the background color <strong>b</strong>. Therefore, the new rendering method is given by
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								$$
								\hat{C}(r) = \sum_{i=1}^{N_c} T_i \left(1 - \exp\left(-\sigma_i\delta_i\right)\right)\mathbf{c}_i + T_{N_c+1}\mathbf{b}
								$$
							</div>
							<p>
								Result if changing background color to light blue:
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/lego_reconstruction_colored_bg.gif" alt="Colored Background" /></span>
							</div>
							<p>
								and pure white:
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/lego_reconstruction_white_bg.gif" alt="White Background" /></span>
							</div>
							<p>
								Notice that there are some black dots in the background, I think that's because the model doesn't predict well at those positions.
							</p>
							<hr>
							<h3 id="rendering-the-depth-map">Rendering the Depth Map</h3>
							<p>
								If we know the point is given by \(p = r_o + tr_d\), we can compute its depth using
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								$$
								z = ||p - r_o|| = t||r_d|| = t
								$$
							</div>
							<p>
								which allows us to render the depth map from any given angle.
							</p>
							<p>
								Here is an example result of doing so
							</p>
							<div style="display: flex; align-items: center; justify-content: center;">
								<span class="image featured"><img src="./images/lego_reconstruction_depth.gif" alt="Depth Map" /></span>
							</div>
						</div>
					</article>
				</div>
				

				<!-- Footer -->
					<section id="footer">
						<ul class="icons">
							<li><a href="https://www.instagram.com/billxiao1121/" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
							<li><a href="../../email_contact/email.html" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
							<li><a href="https://github.com/HaoyueXiao" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
							<li><a href="https://www.linkedin.com/in/haoyuexiao-b6810124b/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
							<li><a href="#" class="icon brands fa-weixin"><span class="label">WeChat</span></a></li>
						</ul>
						<p class="copyright">&copy; Untitled. Design: <a href="http://html5up.net">HTML5 UP</a>. Images: <a href="http://unsplash.com">Unsplash</a>.</p>
					</section>

			</div>

		<!-- Scripts -->
			<script src="./assets/js/jquery.min.js"></script>
			<script src="./assets/js/browser.min.js"></script>
			<script src="./assets/js/breakpoints.min.js"></script>
			<script src="./assets/js/util.js"></script>
			<script src="./assets/js/main.js"></script>

	</body>
</html>
