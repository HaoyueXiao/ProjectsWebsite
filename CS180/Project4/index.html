<!DOCTYPE HTML>
<!--
	Future Imperfect by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Panorama</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="./assets/css/main.css" />
		<script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
		<!-- Lightweight client-side loader that feature-detects and load polyfills only when necessary -->
		<script src="https://cdn.jsdelivr.net/npm/@webcomponents/webcomponentsjs@2/webcomponents-loader.min.js"></script>
		<script type="module" src="https://cdn.jsdelivr.net/npm/zero-md@3?register"></script>
		<script type="text/javascript" async
			src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js">
		</script>
	</head>
	<body class="single is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
				<header id="header">
					<h1><a href="https://haoyuexiao.github.io/">Haoyue Xiao</a></h1>
					<nav class="links">
						<ul>
							<li><a href="https://haoyuexiao.github.io/education/education.html">Education</a></li>
							<li><a href="https://haoyuexiao.github.io/projects/projects.html">Projects</a></li>
							<li><a href="https://haoyuexiao.github.io/cv/CV.htmll">CV</a></li>
							<li><a href="https://haoyuexiao.github.io/personal/personal.html">Personal</a></li>
						</ul>
					</nav>
					<nav class="main">
						<ul>
			
							<li class="menu">
								<a class="fa-bars" href="#menu">Menu</a>
							</li>
						</ul>
					</nav>
				</header>

				<!-- Menu -->
					<section id="menu">


						<!-- Links -->
						<section>
							<ul class="links">
								<li>
									<a href="https://haoyuexiao.github.io/education/education.html">
										<h3>Education</h3>
									</a>
								</li>
								<li>
									<a href="https://haoyuexiao.github.io/projects/projects.html">
										<h3>Projects</h3>
									</a>
								</li>
								<li>
									<a href="https://haoyuexiao.github.io/cv/CV.html">
										<h3>CV</h3>
									</a>
								</li>
								<li>
									<a href="https://haoyuexiao.github.io/personal/personal.html">
										<h3>Personal</h3>

									</a>
								</li>
							</ul>
						</section>

						<!-- ## menu -->
						<section>
							<h3>Contents</h3>
							<ul class="toc">
            <li>
                <a href="#part-A">Part A: Panorama Stitching</a>
                <ul>
                    <li><a href="#part-1-shot-the-pictures">Part 1: Shot the Pictures</a></li>
                    <li><a href="#part-2-recover-homographies">Part 2: Recover Homographies</a></li>
                    <li><a href="#part-3-rectification">Part 3: Rectification</a></li>
                    <li><a href="#part-4-panorama-blending">Part 4: Panorama Blending</a></li>
                    <li>
                        <a href="#bells-whistles">Bells &amp; Whistles</a>
                        <ul>
                            <li><a href="#rotational-model">3D Rotation Model</a></li>
                        </ul>
                    </li>
                </ul>
            </li>
            <li>
                <a href="#cs180-project-4-part-b">Part B: Automatic Feature Descriptor Extraction and Matching</a>
                <ul>
                    <li><a href="#automatic-panorama-creation">Automatic Panorama Creation</a></li>
                    <li><a href="#introduction">Introduction</a></li>
                    <li><a href="#harris-corner-detector">Harris Corner Detector</a></li>
                    <li><a href="#adaptive-non-maximum-suppression">Adaptive Non-maximum Suppression</a></li>
                    <li><a href="#feature-descriptor-extractor">Feature Descriptor Extractor</a></li>
                    <li><a href="#feature-matching">Feature Matching</a></li>
                    <li><a href="#random-sample-consensus-ransac">Random Sample Consensus (RANSAC)</a></li>
                    <li><a href="#panorama-stitching">Panorama Stitching</a></li>
                    <li>
                        <a href="#bells-whistles-multiscale-oriented-patches-scale-and-orientation-invariant">Bells &amp; Whistles: Multiscale Oriented Patches</a>
                        <ul>
                            <li><a href="#multiscale-harris-detection">Multiscale Harris Detection</a></li>
                            <li><a href="#multiscale-anms">Multiscale ANMS</a></li>
                            <li><a href="#feature-descriptor-extraction-1">Feature Descriptor Extraction</a></li>
                            <li><a href="#feature-matching-1">Feature Matching</a></li>
                        </ul>
                    </li>
                </ul>
            </li>
        </ul>
						</section>
						

					<!-- Actions -->
						<section>
							<ul class="actions stacked">
								<li><a href="https://haoyuexiao.github.io/email_contact/email.html" class="button large fit">Contact</a></li>
							</ul>
						</section>

				</section>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<article class="post">
								<header>
									<div class="title">
										<h2><a href="https://haoyuexiao.github.io/ProjectsWebsite/CS180/Project4/"> Panorama</a></h2>
										<p><a href="https://inst.eecs.berkeley.edu/~cs180/fa24/hw/proj4/">CS 180 </a> project 4</p>
									</div>
									<div class="meta">
										<time class="published" datetime="2024-10-19">October 19, 2024</time>
										<a href="https://haoyuexiao.github.io/" class="author"><span class="name">Haoyue Xiao</span><img src="./images/avatar.png" alt="avatar" /></a>
									</div>
								</header>
								<div style="display: flex; align-items: center; justify-content: center;">
									<span class="image featured"><img src="./images/lr_para.jpg" alt="" /></span>
								</div>
								<div style="padding-left:10px; padding-right:10px">
									<h2 id="part-A">Part A: Panorama Stiching</h3>
									<div style="padding-left:10px; padding-right:10px">
										<h3 id="part-1-shot-the-pictures">Part 1: Shot the Pictures</h3>
										<p>I shot 6 images using a Sony A7R4 camera with a 50mm lens and a tripod. I ensured that each photo has about 40% of overlapped area and can find corresponding points in each image.</p>
										<p>Here are the images for demonstration:</p>
										<div style="display: flex; align-items: center; justify-content: center;">
											<span class="image featured"><img src="./images/demo.png" alt="Demonstration Images" /></span>
										</div>
										
										<div style="display: flex; align-items: center; justify-content: center;">
											<span class="image featured"><img src="./images/moffit.png" alt="Moffit Image" /></span>
										</div>
										
										<div style="display: flex; align-items: center; justify-content: center;">
											<span class="image featured"><img src="./images/coast.png" alt="Coast Image" /></span>
										</div>
										
										<h3 id="part-2-recover-homographies">Part 2: Recover Homographies</h3>
										<p>I used the tool I implemented in Project 3 to select 10 correspondence points between the third and the fourth images, and here are the points:</p>
										<div style="display: flex; align-items: center; justify-content: center;">
											<span class="image featured"><img src="./images/points.png" alt="Correspondence Points" /></span>
										</div>
										
										<p>Now, given the corresponding points in the homogeneous system</p>
										
										<div style="display: flex; align-items: center; justify-content: center;">
											$$
											p = \begin{bmatrix}p_x^{(1)}\\ p_y^{(1)}\\1\end{bmatrix} \quad q = \begin{bmatrix}q_x^{(1)}\\ q_y^{(1)}\\1\end{bmatrix}
											$$
										</div>
										
										<p>and the homography matrix</p>
										
										<div style="display: flex; align-items: center; justify-content: center;">
											$$
											H = \begin{bmatrix}h_1&h_2&h_3\\ h_4&h_5&h_6 \\h_7&h_8&1\end{bmatrix}
											$$
										</div>
										
										<p>We have</p>
										
										<div style="display: flex; align-items: center; justify-content: center;">
											$$
											Hp = \begin{bmatrix}
											p_x^{(1)}h_1+p_y^{(1)}h_2+h_3\\
											p_x^{(1)}h_4+p_y^{(1)}h_5+h_6\\
											p_x^{(1)}h_7+p_y^{(1)}h_8+1
											\end{bmatrix}
											$$
										</div>
										
										<p>Since we are in the homogeneous system, we should divide the first 2 components by the third to get the normalized value in the Cartesian coordinate, and equate that to the target points:</p>
										
										<div style="display: flex; align-items: center; justify-content: center;">
											$$
											\begin{cases}
											\frac{p_x^{(1)}h_1+p_y^{(1)}h_2+h_3}{p_x^{(1)}h_7+p_y^{(1)}h_8+1} = q_x^{(1)}\\
											\frac{p_x^{(1)}h_4+p_y^{(1)}h_5+h_6}{p_x^{(1)}h_7+p_y^{(1)}h_8+1} = q_y^{(1)}
											\end{cases}
											$$
										</div>
										
										<p>Simplifying this gives:</p>
										
										<div style="display: flex; align-items: center; justify-content: center;">
											$$
											\begin{cases}
											p_x^{(1)}h_1+p_y^{(1)}h_2+h_3 + 0h_4 + 0h_5 + 0h_6 - q_x^{(1)}p_x^{(1)}h_7 - q_x^{(1)}p_y^{(1)}h_8 = q_x^{(1)}\\
											0h_1 + 0h_2 + 0h_3 + p_x^{(1)}h_4+p_y^{(1)}h_5+h_6 - q_y^{(1)}p_x^{(1)}h_7 - q_y^{(1)}p_y^{(1)}h_8 = q_y^{(1)}
											\end{cases}
											$$
										</div>
										
										<p>Which further suggests</p>
										
										<div style="display: flex; align-items: center; justify-content: center;">
											$$
											\begin{bmatrix}
											p_x^{(1)} & p_y^{(1)} & 1 & 0 & 0 & 0 & -q_x^{(1)}p_x^{(1)} & -q_x^{(1)}p_y^{(1)}\\
											0 & 0 & 0 & p_x^{(1)} & p_y^{(1)} & 1 & -q_y^{(1)}p_x^{(1)} & -q_y^{(1)}p_y^{(1)}
											\end{bmatrix}
											\begin{bmatrix}h_1\\h_2\\h_3\\h_4\\h_5\\h_6\\h_7\\h_8\end{bmatrix} = \begin{bmatrix}q_x^{(1)}\\ q_y^{(1)}\end{bmatrix}
											$$
										</div>
										
										<p>Therefore, to solve for \( H \) that accommodates each \( p, q \), we integrate all the points into the matrix</p>
										
										<div style="display: flex; align-items: center; justify-content: center;">
											$$
											A = \begin{bmatrix}
											p_x^{(1)} & p_y^{(1)} & 1 & 0 & 0 & 0 & -q_x^{(1)}p_x^{(1)} & -q_x^{(1)}p_y^{(1)}\\
											0 & 0 & 0 & p_x^{(1)} & p_y^{(1)} & 1 & -q_y^{(1)}p_x^{(1)} & -q_y^{(1)}p_y^{(1)}\\
											\vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
											p_x^{(N)} & p_y^{(N)} & 1 & 0 & 0 & 0 & -q_x^{(N)}p_x^{(N)} & -q_x^{(N)}p_y^{(N)}\\
											0 & 0 & 0 & p_x^{(N)} & p_y^{(N)} & 1 & -q_y^{(N)}p_y^{(N)} & -q_y^{(N)}p_y^{(N)}
											\end{bmatrix}
											$$
										</div>
										
										<p>and</p>
										<div style="display: flex; align-items: center; justify-content: center;">
											$$
											b = [q_x^{(1)}, q_y^{(1)}, \cdots, q_x^{(N)}, q_y^{(N)}]^T
											$$
										</div>
										
										<p>and solve \( Ah = b \) using the least squares solver.</p>
										
										<p>Using this approach, we are able to find the \( H \) that transforms <code>pt1</code> to <code>pt2</code> effectively.</p>
										
										<p>Transforming <code>para_3</code> to the points in <code>para_4</code> gives the following image:</p>
										<div style="display: flex; align-items: center; justify-content: center;">
											<span class="image featured"><img src="./images/transform.png" alt="Transformed Image" /></span>
										</div>
										
										<h3 id="part-3-rectification">Part 3: Rectification</h3>
										<p>To ensure that our homography works correctly, we can try to rectify an image with a known rectangular object to a standard rectangular shape. Here's a photo of my laptop on my bed.</p>
										<div style="display: flex; align-items: center; justify-content: center;">
											<span class="image featured"><img src="./images/laptop.png" alt="Laptop on Bed" /></span>
										</div>
										
										<p>Due to perspective, it's not standardly rectangular. We can use homography to correct it:</p>
										<div style="display: flex; align-items: center; justify-content: center;">
											<span class="image featured"><img src="./images/laptop_c.png" alt="Rectified Laptop Image" /></span>
										</div>
										
										<div style="display: flex; align-items: center; justify-content: center;">
											<span class="image featured"><img src="./images/moffit_rec.png" alt="Moffit Library Rectification" /></span>
										</div>
										
										<h3 id="part-4-panorama-blending">Part 4: Panorama Blending</h3>
										<p>Now, given image 1 and image 2, we can use the computed homography to wrap <code>img1</code> perspectively to the plane of <code>img2</code>, but we still need to blend the two images to create a seamless panorama.</p>
										
										<p>Here are the steps to achieve this process:</p>
										<ol>
											<li>Compute the homography \( H \) from <code>pts1</code> and <code>pts2</code>.</li>
											<li>Transform the corner points of <code>img1</code> to get the position of <code>img1</code> in the plane of <code>img2</code>. Use this to get the new canvas size and compute the translation <code>T</code> needed to move the images to the new coordinate.</li>
											<li>Use <code>T</code> and <code>H</code> to wrap <code>img1</code> onto the plane of <code>img2</code>.</li>
											<li>Use threshold masking to find the mask of <code>wrapped_img1</code> and <code>wrapped_img2</code> respectively.</li>
											<li>Compute the alpha masking using distance to the background. Here's one example:</li>
										</ol>
										<div style="display: flex; align-items: center; justify-content: center;">
											<span class="image featured"><img src="./images/alpha.png" alt="Alpha Masking Example" /></span>
										</div>
										<ol start="6">
											<li>Use the algorithm from Project 2, with the distanced alpha as the mask, to blend the images using multi-band blending.</li>
										</ol>
										
										<p>Here's the result of combining <code>para_3</code> and <code>para_4</code>:</p>
										<div style="display: flex; align-items: center; justify-content: center;">
											<span class="image featured"><img src="./images/paranoma.jpg" alt="Combined Panorama" /></span>
										</div>
										
										<p>And here's the full panorama of combining all 6 images together:</p>
										<div style="display: flex; align-items: center; justify-content: center;">
											<span class="image featured"><img src="./images/full.jpg" alt="Full Panorama" /></span>
										</div>
										
										<p>The result of merging the moffit images:</p>

										<div style="display: flex; align-items: center; justify-content: center;">
											<span class="image featured"><img src="./images/moffit_para.png" alt="Moffit Library Panorama" /></span>
										</div>
										
										<p>The result of merging 3 images on the Maine coast:</p>

										<div style="display: flex; align-items: center; justify-content: center;">
											<span class="image featured"><img src="./images/paranoma_coast_2.jpg" alt="Maine Coast Panorama" /></span>
										</div>
										
										<h3 id="bells-whistles">Bells & Whistles</h3>
										
										<h4 id="rotational-model">3D Rotational Model</h4>
										<p>
											Assume the focal length \(f\) is fixed, and the image is captured by purely rotating about the optical center. Under this assumption, we can model the scene such that the points in the images can be aligned using a purely 3D rotation. This simplification reduces the degrees of freedom in our model, making the computation more robust and efficient.
											Specifically, we compute the homography using the formula
										</p>
										<div style="display: flex; align-items: center; justify-content: center;">
											$$
											H = KRK^{-1}
											$$
										</div>
										<p>
											where \( K \) is the camera intrinsic matrix and \( R \) is the rotation matrix. The camera intrinsic matrix is typically defined as
										</p>
										<div style="display: flex; align-items: center; justify-content: center;">
											$$
											K = \begin{bmatrix}
											f & 0 & c_x \\
											0 & f & c_y \\
											0 & 0 & 1
											\end{bmatrix}
											$$
										</div>
										
										<p>
											Since I use a full-frame camera with a 50mm lens and the image has resolution \((H, W)\), we can compute the focal length by
										</p>
										<div style="display: flex; align-items: center; justify-content: center;">
											$$
											\text{pixel}_x = \frac{\text{SensorSize}_w}{W} \quad \text{pixel}_y = \frac{\text{SensorSize}_h}{H}
											$$
										</div>
										<div style="display: flex; align-items: center; justify-content: center;">
											$$
											f_x = \frac{50}{\text{pixel}_x} \quad f_y = \frac{50}{\text{pixel}_y}
											$$
										</div>
										<div style="display: flex; align-items: center; justify-content: center;">
											$$
											c_x = \frac{W}{2} \quad c_y = \frac{H}{2}
											$$
										</div>
										<p>
											This accounts for \( K \).
										</p>
										
										<p>
											To find \( R \), we can use the Kabsch algorithm on the camera-intrinsic-normalized \( \text{pts1} \) and \( \text{pts2} \). That means, we first normalize \( P \in \mathbb{R}^{N \times 3} \) and \( Q \) by \( \bar{P} = (K^{-1}P^{T})^{T} \), and similarly for \( \bar{Q} \), because the homography \( H = KRK^{-1} \) performs the camera normalization first.
										</p>
										
										<p>
											Then, we want to find the rotation \( R \) that minimizes the cross-correlation between \( \bar{P} \) and \( \bar{Q} \). From linear algebra, we know that this involves performing the singular value decomposition (SVD) of the covariance matrix formed by the normalized point sets \( \bar{P} \) and \( \bar{Q} \). By decomposing this covariance matrix, we obtain the matrices \( U \), \( \Sigma \), and \( V^\top \), which are then used to construct the optimal rotation matrix \( R \) as
										</p>
										<div style="display: flex; align-items: center; justify-content: center;">
											$$
											R = V U^T.
											$$
										</div>
										<p>
											We aim to find the rotation matrix \(R\) that minimizes the cross-correlation between \(\bar{P}\) and \(\bar{Q}\). From linear algebra, this involves performing the SVD of the covariance matrix formed by the normalized point sets (\(\bar{P}^T\bar{Q}\)). By decomposing this covariance matrix, we obtain the matrices \(U\), \( \Sigma \), \( V^T\), which are then used to construct the optimal rotation matrix \(R\) as:
										</p>
										<div style="display: flex; align-items: center; justify-content: center;">
											$$
											R = V E U^T
											$$
										</div>
										<p>
											where \( E = \mathrm{diag}(1,1,\det(VU^T)) \).
											This rotation matrix aligns the points in \(\bar{P}\) with those in \(\bar{Q}\) by minimizing the alignment error.
										</p>
										
										<p>
											Here are the results of using the rotational model to align <code>para_3</code> and <code>para_4</code>:
										</p>
										<div style="display: flex; align-items: center; justify-content: center;">
											<span class="image featured"><img src="./images/rotate_3_4.png" alt="Rotational Model Alignment" /></span>
										</div>
										
										<p>It's obvious that there is less distortion in the left region.</p>
										
										<p>Here's also the rotational result of merging two images about the Maine coast:</p>
										<div style="display: flex; align-items: center; justify-content: center;">
											<span class="image featured"><img src="./images/coast_R.png" alt="Maine Coast Rotational Alignment" /></span>
										</div>
										
										<p>The alignment is more defective because I wasn't using a tripod when taking these photos.</p>
									</div>
									<br>
									<hr>
									<br>
									<div style="padding-left:10px; padding-right:10px">
										<h2 id="cs180-project-4-part-b">Part B: Automatic Feature Descriptor Extraction and Macthing</h2>
										<div style="padding-left:10px; padding-right:10px">
											<h3 id="automatic-panorama-creation">Automatic Panorama Creation</h3>
											<h4 id="introduction">Introduction</h4>
											<p>In the last section, we annotate keypoints by hands and use them to compute the homography matrix \(H\) and wrap one image to the plane of the other. In practice, this process is both tedious and inaccurate. When selecting points from a small window using pointers, we can accidentally select points that drift by a few pixels, leading to non-exact matchings.</p>
											<p>In this project, I will build a pipeline to automate this process of keypoints annotation and build matchings between two images we intend to merge.</p>
											<hr>
											<h4 id="harris-corner-detector">Harris Corner Detector</h4>
											<p>For the initial search of interest points, we use the harris corner detector which, as its name suggests, search points that are at the "corners." Given window size \(w\), image \(I(x,y)\), we:</p>
											<ol>
												<li>Apply gaussian blur with \(\sigma_b\), then find the derivative w.r.t \(x\) and \(y\), respectively \(I_x\) and \(I_y\).</li>
												<li>Find \(I_{xx} = I_x^2\), \(I_{yy} = I_y^2\), \(I_{xy} = I_xI_y\).</li>
												<li>Given point \((u,v)\), the sliding window \(\mathcal{W} = [u - w/2:u+ w/2, v - w/2, v+w/2]\). Compute \(S_{xx} = \sum_{(x,y)\in \mathcal{W}}I_{xx}\), \(S_{yy} = \sum_{(x,y)\in \mathcal{W}}I_{yy}\), \(S_{xy} = \sum_{(x,y)\in \mathcal{W}}I_{xy}\).</li>
												<li>For each (u,v), compute the harris response \(h = \frac{S_{xx}S_{yy} - S_{xy}^2}{S_{xx}S_{yy}}\).</li>
												<li>Find all points such that the harris response is above the threshold.</li>
											</ol>
											<p>Here's the result of running harris corner detection on the images of moffit libraries, where I also applied a non-maximum filtering with kernel size of 10 pixels and discard 20 pixels near the edge.</p>
											<div style="display: flex; align-items: center; justify-content: center;">
												<span class="image featured"><img src="./images/harris_moffit.png" alt="Harris Moffit" /></span>
											</div>
											<hr>
											<h4 id="adaptive-non-maximum-suppression">Adaptive Non-maximum Suppression</h4>
											<p>There are too many points in the results of harris corner detection, and in many cases, a very distinctive corner may have many detected points around it. We somehow want the points to be more evenly distributed. The algorithm of adaptive non-maximum suppression aims to preserve points that are the local max of its region and filter out the less representative ones. Here's the algorithm description using a KD Tree:</p>
											<ol>
												<li>Given harris responses: <code>coords</code> (N,2) and the corresponding response values <code>h</code> (N,1), we sort the coords in reversed order according to \(h\).</li>
												<li>Maintain a radii recorder for each point in <code>coords</code> that represents the maximum radius such that this point has the strongest harris response within the circle of this radius.</li>
												<li>Initialize an empty KD tree.</li>
												<li>For each point in the sorted array, if it's the first element, mark its radii as <code>inf</code> and insert it into the KD tree. For later points, we query the tree the distance to its nearest neighbor, <code>r</code>, and make it the radii of that point. Then, we insert the point into the tree. (The reason this works is because at any time, the tree only has points with stronger response than the point being queried)</li>
												<li>Sort the <code>coords</code> according to their radii, and take the first <code>n_points</code> coords.</li>
											</ol>
											<p>Here's the result of running ANMS on the moffit pictures. It's obvious that the points are pretty evenly distributed. I only took 500 points.</p>
											<div style="display: flex; align-items: center; justify-content: center;">
												<span class="image featured"><img src="./images/moffit_ANMS.png" alt="Moffit ANMS" /></span>
											</div>
											<hr>
											<h4 id="feature-descriptor-extractor">Feature Descriptor Extractor</h4>
											<p>Then, we need a way to effectively compare the keypoints in two images in order to find best matches. Here, for each point in the ANMS result, I took a \(40\times 40\) patch around that pixel, apply a low-pass filter and then resize it to a \(8\times 8\) patch. Then, I flattened it and normalize it by its mean and variance. This creates a feature descriptor for the interest point. Here are some patches before the resize and normalization for visualization purposes:</p>
											<div style="display: flex; align-items: center; justify-content: center;">
												<span class="image featured"><img src="./images/patch_original_comparison.png" alt="Original Patches Comparison" /></span>
											</div>
											<div style="display: flex; align-items: center; justify-content: center;">
												<span class="image featured"><img src="./images/more_patches.png" alt="More Patches" /></span>
											</div>
											<hr>
											<h4 id="feature-matching">Feature Matching</h4>
											<p>After deriving the feature descriptors for both images, we need to establish correspondences between the points in the two images. Here, I estimate the similarity between two patches by computing the \(l_2\) distance between the two descriptors. The algorithm description is:</p>
											<ol>
												<li>Create two KD Trees respectively for <code>descriptors1</code> and <code>descriptors2</code>. And two arrays keeping track of the best matches.</li>
												<li>For each descriptor in <code>descriptors1</code>, query the <code>KDTree2</code> about its nearest neighbor in image 2 <code>(nn1, d1)</code> and the second nearest neighbor <code>(nn2, d2)</code>. Then, we only append <code>nn1</code> to the best matches for image 1 if:
													<ul>
														<li>(1) <code>d1</code> &lt; <code>thresh_abs</code>. That means the two patches are similar enough</li>
														<li>(2) <code>d1/(d2+1e-6)</code> &lt; <code>thresh_ratio</code>. This is used to implement Lowe's ratio test that we take the point only when the nearest neighbor is much better than the second to best.</li>
													</ul>
													We do the same for each descriptor in <code>descriptors2</code>.
												</li>
												<li>For the matching results in <code>match1</code> and <code>match2</code>, we add a pair <code>(pt1, pt2)</code> to the final result if the best match of <code>pt1</code> is <code>pt2</code> and vice versa. This means that we only accept double selection.</li>
											</ol>
											<p>Here's the result when I set <code>thresh_abs=5</code> and <code>thresh_ratio=0.7</code>. We have 27 matches, and it's notable that the outliers have dramatically decreased and most points look like good matchings.</p>
											<div style="display: flex; align-items: center; justify-content: center;">
												<span class="image featured"><img src="./images/moffit_matching.png" alt="Moffit Matching" /></span>
											</div>
											<hr>
											<h4 id="random-sample-consensus-ransac">Random Sample Consensus (RANSAC)</h4>
											<p>However, even after all these procedures, there are still outliers existing in the matchings, and it's risky if we just compute the homographies using all of them. To address this issue, I used the RANSAC algorithm to create a much more robust homography <code>H</code>. The algorithm follows the guidelines below:</p>
											<ol>
												<li>The algorithm has inputs <code>matching</code> (result from last step), <code>n_loop</code> (number of iteration loops to run), <code>epsilon</code> (criterion for determining if a point is an inlier or an outlier)</li>
												<li>Initialize <code>best_H</code> to be the trivial \(3\times 3\) matrix, and the <code>inliers</code> to be an empty array.</li>
												<li>Repeat for <code>n_loop</code> times, each time we randomly pick 4 points, without replacement, from <code>matching</code>, and solve an exact homography <code>H</code> using these points. Then, we transform points from \(I_A\) using <code>H</code>, and compute the \(l_2\) distance between transformed points and the matching points in \(I_B\). Record the points such that the distance is smaller than <code>epsilon</code>, call them inliers.</li>
												<li>If the number of inliers of this round is larger than the number in the global <code>inliers</code>, we change the global pointer to both the inliers and <code>H</code> from this round.</li>
												<li>After all iterations, recompute <code>H</code> using the final set of <code>inliers</code>, and return <code>H</code>.</li>
											</ol>
											<hr>
											<h4 id="panorama-stitching">Panorama Stitching</h4>
											<p>Finally, with the <code>H</code> derived from RANSAC, we can use the panorama stitching pipeline built in part A to merge two images into a panorama. Here are some comparisons between hand-annotated panoramas and the panoramas created using the pipeline we just introduced:</p>
											<div style="display: flex; align-items: center; justify-content: center;">
												<span class="image featured"><img src="./images/panorama_moffit.jpg" alt="Panorama Moffit" /></span>
											</div>
											<div style="display: flex; align-items: center; justify-content: center;">
												<span class="image featured"><img src="./images/moffit_comparison.png" alt="Moffit Comparison" /></span>
											</div>
											<div style="display: flex; align-items: center; justify-content: center;">
												<span class="image featured"><img src="./images/panorama_coast.jpg" alt="Panorama Coast" /></span>
											</div>
											<div style="display: flex; align-items: center; justify-content: center;">
												<span class="image featured"><img src="./images/coast_comparison.png" alt="Coast Comparison" /></span>
											</div>
											<p>The roof image (merging 5 pictures) using the automated pipeline:</p>
											<div style="display: flex; align-items: center; justify-content: center;">
												<span class="image featured"><img src="./images/panorama_roof_1_5.jpg" alt="Panorama Roof" /></span>
											</div>
											<p>The roof image using hand annotated keypoints in part A:</p>
											<div style="display: flex; align-items: center; justify-content: center;">
												<span class="image featured"><img src="./images/roof_hand.jpg" alt="Roof Hand Annotated" /></span>
											</div>
											<hr>
											<h3 id="bells-whistles-multiscale-oriented-patches-scale-and-orientation-invariant">Bells &amp; Whistles: Multiscale Oriented Patches (Scale and Orientation Invariant)</h3>
											<p>In the paper of <a href="https://inst.eecs.berkeley.edu/~cs180/fa24/hw/proj4/Papers/MOPS.pdf">MOPS</a>, the author proposed the method of keypoint matching that is invariant to both scale and orientation. In this project, I also implemented this pipeline.</p>
									
											<h4 id="multiscale-harris-detection">Multiscale Harris Detection</h4>
											<p>In the multiscale harris detection, we detect harris corners on different scales of the image across a pyramid to be robust to scale variations. At each layer, we do the similar detection as a single layer harris detection, but this time, for each interest point, we also fit the points around the interest point (with <code>w_size=3</code>) and their harris responses to a 2D quadratic function, and use least squares to find the minima. We use this technique to refine the interest point to subpixel precision for higher computation accuracy.</p>
											<div style="display: flex; align-items: center; justify-content: center;">
												<span class="image featured"><img src="./images/multi_harris_moffit.png" alt="Multiscale Harris Moffit" /></span>
											</div>
											<p>It's worthy noting that in order to make the features rotational invariant, we also estimate the dominate orientation of each interest point. Specifically, we find the derivatives of gaussian with width \(\sigma_o = 4.5\). Given interest point \((u,v)\), we have:</p>
											<div style="display: flex; align-items: center; justify-content: center;">
												\( u = \begin{bmatrix}I_x(u,v)\\I_y(u,v) \end{bmatrix} \)
											</div>
											<p>and</p>
											<div style="display: flex; align-items: center; justify-content: center;">
												\(\mathrm{orientation}  = \frac{u}{||u||} = \begin{bmatrix}\cos \theta \\ \sin \theta\end{bmatrix}\)
											</div>
											<p>then</p>
											<div style="display: flex; align-items: center; justify-content: center;">
												\(\theta = \arctan \frac{u_1}{u_2}\)
											</div>
											<p>Here's a picture showing the orientations under different scales:</p>
											<div style="display: flex; align-items: center; justify-content: center;">
												<span class="image featured"><img src="./images/o_1.png" alt="Orientation 1" /></span>
											</div>
											<div style="display: flex; align-items: center; justify-content: center;">
												<span class="image featured"><img src="./images/o_2.png" alt="Orientation 2" /></span>
											</div>
											<div style="display: flex; align-items: center; justify-content: center;">
												<span class="image featured"><img src="./images/o_3.png" alt="Orientation 3" /></span>
											</div>
											<div style="display: flex; align-items: center; justify-content: center;">
												<span class="image featured"><img src="./images/o_4.png" alt="Orientation 4" /></span>
											</div>
											<p>(The result displayed above are actually after the ANMS)</p>
									
											<h4 id="multiscale-anms">Multiscale ANMS</h4>
											<p>Similarly, we run ANMS on each layer of the scale and here are the result:</p>
											<div style="display: flex; align-items: center; justify-content: center;">
												<span class="image featured"><img src="./images/multi_anms_moffit.png" alt="Multiscale ANMS Moffit" /></span>
											</div>
									
											<h4 id="feature-descriptor-extraction">Feature Descriptor Extraction</h4>
											<p>At each scale level, we still extract a \(40\times 40\) patch (at that scale). But to accommodate rotational invariant extracting, given the orientation \(\theta\), we take the patch along the dominate orientation and then resize it to the \(8\times 8\) smaller patch, flatten and normalize as stated above. Here are some results of the rotated patches (before downsizing and normalizing):</p>
											<div style="display: flex; align-items: center; justify-content: center;">
												<span class="image featured"><img src="./images/mops_patches.png" alt="MOPS Patches" /></span>
											</div>
									
											<h4 id="feature-matching-1">Feature Matching</h4>
											<p>Before running feature matching, we merge the descriptors of all layers into one array of descriptors. Then we run the regular algorithm introduced above.</p>
											<p>Here are the matching result of the moffit library:</p>
											<div style="display: flex; align-items: center; justify-content: center;">
												<span class="image featured"><img src="./images/mops_matching_moffit.png" alt="MOPS Matching Moffit" /></span>
											</div>
											<p>The matching is very successful and, for the same set of thresholds, there are more matchings in MOPS than the single scale methods discussed above.</p>
											<p>After merging the descriptors, we combine interest points of different scales or angles into consideration and allow the algorithm to find the same points under scale or rotation.</p>

											<p>Here's one example of running MOPS on the roof images:</p>
											<div style="display: flex; align-items: center; justify-content: center;">
												<span class="image featured"><img src="./images/roof_MOPS.png" alt="Roof MOPS" /></span>
											</div>
											<p>And the printing log suggests that the algorithm <code>find best homography with 141 inliers out of 313 matches</code>, which is a huge improvement from the previous <code>23 inliers out of 83 matches</code> when doing this using regular methods.</p>

										</div>
									</div>									
									

								</div>
								
								
							</article>

					</div>

				<!-- Footer -->
					<section id="footer">
						<ul class="icons">
							<li><a href="https://www.instagram.com/billxiao1121/" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
							<li><a href="../../email_contact/email.html" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
							<li><a href="https://github.com/HaoyueXiao" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
							<li><a href="https://www.linkedin.com/in/haoyuexiao-b6810124b/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
							<li><a href="#" class="icon brands fa-weixin"><span class="label">WeChat</span></a></li>
						</ul>
						<p class="copyright">&copy; Untitled. Design: <a href="http://html5up.net">HTML5 UP</a>. Images: <a href="http://unsplash.com">Unsplash</a>.</p>
					</section>

			</div>

		<!-- Scripts -->
			<script src="./assets/js/jquery.min.js"></script>
			<script src="./assets/js/browser.min.js"></script>
			<script src="./assets/js/breakpoints.min.js"></script>
			<script src="./assets/js/util.js"></script>
			<script src="./assets/js/main.js"></script>

	</body>
</html>
