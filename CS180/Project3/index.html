<!DOCTYPE HTML>
<!--
	Future Imperfect by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Face Morphing</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="./assets/css/main.css" />
		<script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
		<!-- Lightweight client-side loader that feature-detects and load polyfills only when necessary -->
		<script src="https://cdn.jsdelivr.net/npm/@webcomponents/webcomponentsjs@2/webcomponents-loader.min.js"></script>
		<script type="module" src="https://cdn.jsdelivr.net/npm/zero-md@3?register"></script>
		<script type="text/javascript" async
			src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js">
		</script>
	</head>
	<body class="single is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
				<header id="header">
					<h1><a href="https://haoyuexiao.github.io/">Haoyue Xiao</a></h1>
					<nav class="links">
						<ul>
							<li><a href="https://haoyuexiao.github.io/education/education.html">Education</a></li>
							<li><a href="https://haoyuexiao.github.io/projects/projects.html">Projects</a></li>
							<li><a href="https://haoyuexiao.github.io/cv/CV.html">CV</a></li>
							<li><a href="https://haoyuexiao.github.io/personal/personal.html">Personal</a></li>
						</ul>
					</nav>
					<nav class="main">
						<ul>
			
							<li class="menu">
								<a class="fa-bars" href="#menu">Menu</a>
							</li>
						</ul>
					</nav>
				</header>

				<!-- Menu -->
					<section id="menu">


						<!-- Links -->
						<section>
							<ul class="links">
								<li>
									<a href="https://haoyuexiao.github.io/education/education.html">
										<h3>Education</h3>
									</a>
								</li>
								<li>
									<a href="https://haoyuexiao.github.io/projects/projects.html">
										<h3>Projects</h3>
									</a>
								</li>
								<li>
									<a href="https://haoyuexiao.github.io/cv/CV.html">
										<h3>CV</h3>
									</a>
								</li>
								<li>
									<a href="https://haoyuexiao.github.io/personal/personal.html">
										<h3>Personal</h3>

									</a>
								</li>
							</ul>
						</section>

						<section>
							<h3>Contents</h3> 
							<ul class="toc"> 
								<li><a href="#cs180-project-3-face-morphing">CS180 Project 3 Face Morphing</a></li> 
								<li><a href="#introduction">Introduction</a></li> 
								<li><a href="#part-1-defining-correspondences">Part 1: Defining Correspondences</a></li> 
								<li><a href="#part-2-computing-the-mid-way-face-">Part 2: Computing the "Mid-way Face"</a></li> 
								<li><a href="#part-3-the-morph-sequence">Part 3: The Morph Sequence</a></li> 
								<li><a href="#part-4-the-mean-face-of-a-population">Part 4: The 'Mean Face' of a Population</a></li> 
								<li><a href="#part-5-extrapolation">Part 5: Extrapolation</a></li> 
								<li> <a href="#bells-whistles">Bells & Whistles</a> <ul> 
									<li><a href="#change-gender">Change Gender</a></li> 
									<li> <a href="#automatic-facial-keypoint-detection-using-neural-nets">Automatic Facial Keypoint Detection Using Neural Nets</a> 
										<ul> <li><a href="#result-on-the-300w-dataset">Result on the 300W Dataset</a></li> 
											<li><a href="#transfer-learning-on-dane-s-dataset">Transfer Learning on Dane's Dataset</a></li> 
											<li><a href="#image-facial-annotation">Image Facial Annotation</a></li> 
											<li><a href="#face-swap">Face Swap</a></li> 
										</ul> 
									</li> 
								</ul>
							</li> 
						</ul> 
						</section>


					<!-- Actions -->
						<section>
							<ul class="actions stacked">
								<li><a href="https://haoyuexiao.github.io/email_contact/email.html" class="button large fit">Contact</a></li>
							</ul>
						</section>

				</section>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<article class="post">
								<header>
									<div class="title">
										<h2><a href="https://haoyuexiao.github.io/ProjectsWebsite/CS180/Project2/"> Face Morphing and Facial Keypoints Detection</a></h2>
										<p><a href="https://inst.eecs.berkeley.edu/~cs180/fa24/hw/proj2/">CS 180 </a> project 3.</p>
									</div>
									<div class="meta">
										<time class="published" datetime="2024-10-08">October 8, 2024</time>
										<a href="https://haoyuexiao.github.io/" class="author"><span class="name">Haoyue Xiao</span><img src="./images/avatar.png" alt="avatar" /></a>
									</div>
								</header>
								<div style="display: flex; align-items: center; justify-content: center;">
									<span class="image featured"><img src="./images/titanic.gif" alt="" /></span>
								</div>

								

								

								<div style="padding-left:10px; padding-right:10px">
										<hr>
										<div style="padding-left:10px; padding-right:10px"> <h3 id="introduction">Introduction</h3> 
											<p>Face morphing is a technique that seamlessly blends two or more facial images to create a smooth transition or a new composite face.</p> 
											<p>In this project, I will implement the face morphing in 3 stages: Image triangulation, Affine wrapping of the triangles, and Cross-dissolving morphing.</p>
											<h3 id="part-1-defining-correspondences">Part 1. Defining Correspondences</h3> 
											<p>To effectively morph two images, we will need to find the correspondences between the key facial points and then compute a <code>mean shape</code> of the face. To achieve the morphing, we first warp <code>image A</code> into the <code>mean face</code>, then transform from the <code>mean face</code> to <code>image B</code>.</p>
											<p>The images I use in this part are <code>A: a random photo of Leonardo</code>, and <code>B: the photo downloaded from the project website</code>.</p> 
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/original_image_part1.png" alt="Original Images" /></span> 
											</div> 

											<p>Initially, we will need to manually annotate the photos with keypoints. I use the following rule to annotate the faces:</p> 
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/annotation_rule.png" alt="Annotation Rule" /></span>
											</div> 

											<p>And the result of annotation is shown below:</p>
											 <div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/annotation_result.png" alt="Annotation Result" /></span> 
											</div>

											<p>The triangles are computed using <code>scipy.spatial.Delaunay</code> where it aims to maximize the smallest acute angle in each triangle.</p>
											<p>We find the mean face by computing the mean of keypoints:</p> 

											$$ 
											\text{mean}_{\text{keypoints}} = \frac{1}{2}(\text{keypoints}_{A} + \text{keypoints}_{B}) 
											$$ 

											<p>The result of the mean shape:</p> 
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/mean_shape.png" alt="Mean Shape" /></span> 
											</div> 

											<p>Since this process is particularly inefficient and tedious, I decided to implement an automatic keypoint annotation model. Check <a href="#automatic-facial-keypoint-detection-using-neural-nets">Bells and Whistles</a> for details.</p>
											<hr>
											<h2 id="part-2-computing-the-mid-way-face-">Part 2: Computing the "Mid-way Face"</h2> 
											<p>In this part, I implemented the <code>computeAffine</code> function which, given a list of triangles (obtained from the triangulation in the last part) of the original image, and the triangles of the target image, we compute the affine transformation matrix \(A\) such that \(AP = Q\), where \(P\) is the triangles position matrix (\(3 \times 3\) in homogeneous coordinates), and \(Q\) is the target triangles stacked. \(A\) is an affine transformation, and therefore $$ A = \begin{bmatrix} a & b & c \\ d & e & f \\ 0 & 0 & 1 \\ \end{bmatrix} $$ and $$ A\begin{bmatrix} p^1_x & p^2_x & p^3_x \\ p^1_y & p^2_y & p^3_y \\ 1 & 1 & 1 \\ \end{bmatrix} = \begin{bmatrix} q^1_x & q^2_x & q^3_x \\ q^1_y & q^2_y & q^3_y \\ 1 & 1 & 1 \\ \end{bmatrix} $$ This implies that $$ \begin{bmatrix} p^1_x & p^1_y & 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & p^1_x & p^1_y & 1 \\ p^2_x & p^2_y & 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & p^2_x & p^2_y & 1 \\ p^3_x & p^3_y & 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & p^3_x & p^3_y & 1 \\ \end{bmatrix} \begin{bmatrix} a \\ b \\ c \\ d \\ e \\ f \\ \end{bmatrix} = \begin{bmatrix} q^1_x \\ q^1_y \\ q^2_x \\ q^2_y \\ q^3_x \\ q^3_y \\ \end{bmatrix} $$ </p>
											<p>In <code>computeAffine</code>, we solve for the coefficients for each corresponding pair of triangles and store them in a list. Then, I implement <code>applyAffine</code> that does the following algorithm:</p> 
											<ol> <li>Create a meshgrid of \(x\)-\(y\) values, representing the indices of each pixel of the target image. Convert all numbers to the homogeneous coordinate.</li> 
												<li>Apply the inverse transform of the affine matrix for each pixel to find the location where it originates before affine transform, effectively avoiding holes. Here, we actually use vectorized code to enhance speed.</li> 
												<li>Use bilinear interpolation for each value that lies between pixels.</li> 
											</ol> 
											
											<p>Then, implement the warp method as follows:</p> 
											<ol> <li>Compute the triangle masks for each triangle in the target image, using <code>skimage.draw.polygon</code>. We increase each triangle to be 5% larger (outwards) to create an overlap among triangles and avoid visible borders.</li> 
												<li>Get the affine transformations from the original image to the target.</li> 
												<li>For each triangle, apply the affine transformation on the image, then mask the result with the corresponding triangle mask. Then, use a boolean matrix to override the corresponding portion of the output image.</li> 
											</ol> 
											
											<p>The result of <code>image A</code> and <code>image B</code> warped to their mean shape is:</p> 
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/wrapped_A_B.png" alt="Warped Images" /></span> 
											</div> 
											
											<p>And their averaged mean face is:</p> 
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/mean_face_A_B.png" alt="Mean Face" /></span> 
											</div> 
											<hr>
											<h3 id="part-3-the-morph-sequence">Part 3: The Morph Sequence</h3> 
											<p>In this section, we create a sequence of image morphing and get a smooth transition from <code>image A</code> to <code>image B</code>.</p> 
											<p>To do this, given total time steps \(n\), and at time \(t\), we compute a warp coefficient \(\phi\) and dissolve coefficient \(\rho\) to be \(\frac{t}{n-1}\). Then, compute the target face to be $$ I_M = (1 - \phi) \cdot I_A + \phi \cdot I_B $$ Then, we warp each image to this middle image, getting \(I_A'\) and \(I_B'\), and mix them according to $$ I = (1 - \rho) \cdot I_A' + \rho \cdot I_B' $$ In this case, at time step 0, the result will be purely <code>image A</code>, and at time \(\frac{n}{2}\), it will be the middle face, and at \(n - 1\), it will be the <code>image B</code>, effectively morphing from A to B.</p> 
											<p>I collect results for all time steps (<code>n = 45</code>) in order, and play them using <code>fps = 30</code>, and here's the video:</p> 
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/morph_video.gif" alt="Morph Video" /></span> 
											</div> 
											<hr>
											<h3 id="part-4-the-mean-face-of-a-population">Part 4: The 'Mean Face' of a Population</h3> 
											<p>In this section, we will work on this <a href="https://web.archive.org/web/20210305094647/http://www2.imm.dtu.dk/~aam/datasets/datasets.html">dataset</a> with annotated facial keypoints. Some example keypoints are shown below:</p> 
											
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/dane_examples.png" alt="Example Keypoints" /></span> 
											</div> 
											
											<p>(I add 12 extra points to better warp the backgrounds)</p> 
											<p>Again, I compute the mean shape of all the points and do a triangulation on that, then warp all images to this mean shape. Some examples are:</p> 
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/wrap_contrast.png" alt="Warped Examples" /></span> 
											</div> 
											
											<p>After all images are warped to the same shape, we can compute the mean face:</p> 
											
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/pop_mean_face.png" alt="Population Mean Face" /></span> 
											</div> 
											
											<p>Notice that the skin is smooth, and that's because all the imperfections of the individuals are smoothed out in average.</p> 
											<p>Now, we annotate the previously used photo of Leonardo (using the automatic annotation model I implement in <a href="#automatic-facial-keypoint-detection-using-neural-nets">Bells and Whistles</a>):</p> 
											
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/mean_face_leonardo.png" alt="Annotated Leonardo" /></span> 
											</div> 
											
											<p>Warping the mean face to Leonardo gives:</p> 
											
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/wrapped_mean_face.jpg" alt="Warped Mean Face" /></span> 
											</div> 
											
											<p>Warping Leonardo to the mean face gives:</p> 
											
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/wrapped_leonardo.jpg" alt="Warped Leonardo" /></span> 
											</div> 
											<hr>
											<h3 id="part-5-extrapolation">Part 5: Extrapolation</h3> 
											<p>We can enhance the features of Leonardo's face using the formula <code>enhanced = inverse_warp((1 + alpha) * wrapped_leonardo - alpha * mean_face)</code>. When <code>alpha &gt; 1</code>, this is effectively adding the difference between wrapped Leonardo and the mean face multiplied by <code>alpha - 1</code> to the wrapped Leonardo, and then warp it back to Leonardo's geometry. And when <code>0 &lt; alpha &lt; 1</code>, it's making Leonardo more towards the mean face. Here are some results:</p> 
											
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/more_leonardo.png" alt="Enhanced Leonardo" /></span> 
											</div> 
											
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/less_leonardo.png" alt="Less Enhanced Leonardo" /></span> 
											</div> 
											
											<p>Notice that the enhancing effect is not obvious, and that's because in this picture of Leonardo, he actually looks very similar to the mean face in geometry. Therefore, only color enhancement is obvious. In the Bells and Whistles section, I will try more contrasting mean faces.</p> 
											<hr>
											<h3 id="bells-whistles">Bells & Whistles</h3> 
											
											<h4 id="change-gender">Change Gender</h4> 
											<p>To manipulate the gender, I do the same thing in Part 5 by first warping the target to a mean woman's face, then enhance towards the woman side, and then warp it back. As an example, I make the picture of George more like Japanese women:</p> 
											
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/george_japanese_women.png" alt="George and Japanese Woman" /></span> 
											</div> <p>Warped towards each other gives:</p> 
											
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/george_japanese_wrapped.png" alt="Warped George and Japanese Woman" /></span> 
											</div> 
											
											<p>The results of enhancing George towards the feminine side and the masculine side:</p> 
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/george_m_f.png" alt="Enhanced George" /></span> 
											</div> 
											<br>
											<h4 id="automatic-facial-keypoint-detection-using-neural-nets">Automatic Facial Keypoint Detection Using Neural Nets</h4> 
											<p>In my journey of this project, I found it most painful and boring to manually annotate the points of the face. To correctly warp two images, both the number of keypoints and the order of the keypoints must be the same. To better address this problem and more conveniently annotate the facial keypoints, I use a convolutional neural network to learn on the <a href="http://dlib.net/files/data/">300W dataset</a> with around 7000 images and each has 68 keypoints.</p> 
											<p>Some example keypoints are (these keypoints are annotated labels given along with the dataset):</p> 
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/300w_keypoints.png" alt="300W Keypoints" /></span> 
											</div> 
											
											<p>(The images are converted to grayscale after data preprocessing)</p> 
											<p>I use the Xception net introduced in <a href="https://arxiv.org/abs/1610.02357">this paper</a>, and a brief visualization of the net is:</p> 
											<!-- (Insert visualization if available) -->
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/network.png" alt="Warped Mean Face" /></span> 
											</div> 
											 
											<p>I follow some guidelines from <a href="https://github.com/braindotai/Facial-Landmarks-Detection-Pytorch/tree/master">this repo</a> and <a href="https://cs231n.stanford.edu/reports/2016/pdfs/010_Report.pdf">this paper</a>.</p> 
											
											<p>To better generalize on other images, I use data augmentation by randomly rotating, cropping, and offsetting.</p> 
											
											<h5 id="result-on-the-300w-dataset">Result on the 300W Dataset</h5> 
											<p>After training 50 epochs on the 300W dataset, the model is capable of annotating facial landmarks with 68 points:</p> 
											<p>At the beginning, the model gives completely random predictions:</p>
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/300w_epoch0.png" alt="Epoch 0 Results" /></span> 
											</div> 
											
											<p>After a few epochs, it starts to learn facial patterns and generates points that look like a face, but not entirely accurate:</p> 
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/300w_epoch2.png" alt="Epoch 2 Results" /></span> 
											</div> 
											
											<p>After 30 epochs, the model is capable of predicting almost accurate results on most images:</p> 
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/300w_epoch30_2.png" alt="Epoch 30 Results" /></span> 
											</div> 
											
											<h5 id="transfer-learning-on-dane-s-dataset">Transfer Learning on Dane's Dataset</h5> 
											<p>After we have the capable model, we can do cool things like re-annotate the Dane dataset we used in Part 4, and since the 300W dataset has 68 keypoints, it actually provides smoother keypoints on facial landmarks and better morphing performance. Here are some examples of re-annotated Dane datasets:</p> 
											
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/reannotated_dane.png" alt="Re-annotated Dane Dataset" /></span> 
											</div> 
											
											 
											<p>However, if we want to stick with the original number of keypoints, we can use a trick to convert our learned, 68-keypoints model to a 57-keypoint model that fits the Dane's format. The reason we cannot directly train on Dane is that it's too small (37 images) to get good results. So we do training on 300W first and we remove the fully-connected layer that converts <code>hidden_dim=1024</code> to <code>output_dim=2*68</code>, and instead make it <code>nn.Linear(1024, 2*57)</code>. Then, we freeze the convolutional networks to keep the learned weights safe, and only learn the last fully connected layer. This is called transfer learning.</p> 
											<p>After doing this for 30 epochs on the Dane dataset, we have a model that predicts keypoints in Dane's format, and here are some comparisons between original keypoints and predicted keypoints:</p> 
											
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/dane_contrast.png" alt="Dane Keypoints Contrast" /></span> 
											</div> 
											
											<h5 id="image-facial-annotation">Image Facial Annotation</h5> 
											<p>We can also use the model to annotate each frame in a video:</p> 
											
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/titanic.gif" alt="Titanic Annotation" /></span> 
											</div> 
											
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/rick.gif" alt="Video Annotation" /></span> 
											</div> 
											<p style="text-align: center;">There are actually keypoints on their faces but the gif is compressed too badly</p>
											
											<h5 id="face-swap">Face Swap</h5> 
											<p>Given the facial keypoints of two images, we can effectively swap the face of picture 1 tpo picture 2 by applying affine transformation to the region of interests, and mask out other regions.<br> Here's an example of switching the face of Goerge with Leonardo:</p> 
											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/face_swap.png" alt="Video Annotation" /></span> 
											</div> 

											More generally, we can apply this to every frame in a video, and here's the result of face swapping Henry Cavill to Bruce Wayne. The effect is not good but I tried my best :(</p>

											<div style="display: flex; align-items: center; justify-content: center;"> 
												<span class="image featured"><img src="./images/darkknight_face_swap.gif" alt="Video Annotation" /></span>
											</div>

											<p style="text-align: center;">(Implementation of these parts uses high-level functions like OpenCV's <code>ConvexHull</code>, <code>fillConvexPoly</code>, and <code>wrapAffine</code> to handle pictures of difference sizes. I tried to use my customized functions but not working well)</p>
										</div>
										
										
										

								  </div>
							</article>

					</div>

				<!-- Footer -->
					<section id="footer">
						<ul class="icons">
							<li><a href="https://www.instagram.com/billxiao1121/" class="icon brands fa-instagram"><span class="label">Instagram</span></a></li>
							<li><a href="../../email_contact/email.html" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
							<li><a href="https://github.com/HaoyueXiao" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
							<li><a href="https://www.linkedin.com/in/haoyuexiao-b6810124b/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
							<li><a href="#" class="icon brands fa-weixin"><span class="label">WeChat</span></a></li>
						</ul>
						<p class="copyright">&copy; Untitled. Design: <a href="http://html5up.net">HTML5 UP</a>. Images: <a href="http://unsplash.com">Unsplash</a>.</p>
					</section>

			</div>

		<!-- Scripts -->
			<script src="./assets/js/jquery.min.js"></script>
			<script src="./assets/js/browser.min.js"></script>
			<script src="./assets/js/breakpoints.min.js"></script>
			<script src="./assets/js/util.js"></script>
			<script src="./assets/js/main.js"></script>

	</body>
</html>
